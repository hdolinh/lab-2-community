---
title: 'Lab 2: Community'
author: "Halina Do-Linh"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Clustering

## Load and plot `penguins` dataset

```{r}
# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, palmerpenguins, skimr, tibble, tidyverse, h2o, vegan, vegan3d)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("penguins")
```


```{r}
# look at documentation in RStudio
if (interactive())
  help(penguins)

# show data table
datatable(penguins)
```


```{r}
# skim the table for a summary
skim(penguins)
```


```{r}
# remove the rows with NAs
penguins <- na.omit(penguins)

# plot bill length vs width, species naive
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()
```


```{r}
# plot bill length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos
```

## Cluster `penguins` using `kmeans()`

```{r}
# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# show cluster result
penguins_k
```


```{r}
# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)
```

## Question

How many observations could be considered “misclassified” if expecting bill length and width to differentiate between species?

**Answer:** At least 45 observations for the gentoo penguin could be misclassifed since most gentoo observations are in the third cluster. There could also be some misclassification in the third cluster where chinstrap and adelie penguins have been added to that cluster.

```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos
```


## Question

Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.

**Answer:** The observed species plot with the unsupervised `kmeans()` technique visually shows the three species grouped as diagonal circles. The plot with the `kmeans()` technique visually shows the clusters grouped as horizontals. When comparing the two plots it appears the `kmeans()` clusters are mostly similar, but some of the gentoo and chinstrap observations have been predicted to be other species or misclassified.  


## Plot Voronoi diagram of clustered `penguins`

```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

**Task:** Show the Voronoi diagram for fewer (k=2) and more (k=8) clusters to see how assignment to cluster centroids work.

```{r}
# voronoi diagram for k=2
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 2  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```


```{r}
# voronoi diagram for k=8
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 8  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

# Hierarchial Clustering

```{r}
librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```


## Question
What are the rows and columns composed of in the `dune` data frame?

**Answer:** The `dune` data frame are observations of 30 species at 20 sites. The columns represent names of the species, abbreviated to 4+4 letters where the first four letters of the species and the last four are BLANK. 

## Calculate Ecological Distances on `sites`

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```


```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```


```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```


```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

## Question

In your own words, how does Bray Curtis differ from Euclidean distance?

**Answer:** The Bray Curtis distance differs from the Euclidean distance because it restricts the outputs to a range of 0 to 1 (where 0 is most similar and 1 is least similar). Euclidean distance is unrestricted, and is more influenced by abundance and outliers.


## Agglomerative hierarchical clustering on `dune`

```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
```


```{r}
as.matrix(d)[1:5, 1:5]
```


```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)
```

## Question

Which function comes first, vegdist() or hclust(), and why? 

**Answer:** The function `vegdist()` comes first because it is measuring the dissimilarity values (aka distances) before it assigns a cluster. Then you use `hclust()` to create the clusters that make up the dendrogram. 


```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac
```


```{r}
# Dendrogram plot of hc2
plot(hc2, which.plot = 2)
```


## Question

In your own words how does hclust() differ from agnes()?

**Answer:** The `agnes()` function and the `hclust()` funcation are very similar and both build a dendrogram from the bottom up, meaning it builds upon similar paired clusters. `agnes()` provides the agglomerative coefficient, which is useful in describing the strengths of the clustering structure.

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```


```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac
```


```{r}
# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```

## Question
	
Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?

**Answer:** The Ward method provides the best model in terms of Agglomerative Coefficient because it minimizes the total within-cluster variance. 

## Divisive hierarchical clustering on `dune`

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

## Question

In your own words how does agnes() differ from diana()?

**Answer:** `agnes()` does agglomerative clustering (builds from bottom up) and gives a agglomerative coefficient. `diana()` does hierarchical clustering (builds from top to bottom) and provides a divisive coefficient. 

## Determining optimal clusters

```{r}
librarian::shelf(factoextra)

# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

## Question

How do the optimal number of clusters compare between methods for those with a dashed line?

The silhouette method found the optimal number of clusters is three and the gap statistic method found the optimal number of clusters is four. There is not necessarily a definitively clear number of optimal clusters, but 3-4 clusters would be sufficient. 

## Question

In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection?

**Answer:** In dendrogram plots, the relatedness between observations is most determined by the height of their shared connection. The x axis doesn't show a measure distance, it shows which observations are clustered together. 

## Working with dendrograms
```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
```


```{r}
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```


```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```


```{r}
# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```


# Ordination

```{r}
# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
```

## Principal Components Analysis (PCA)

```{r}
# look at data
my_basket
```

## Performing PCA in R

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance
```

```{r}
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

## Question

Why is the pca_method of “GramSVD” chosen over “GLRM”?

**Answer:** `GramSVD` is chosen over `GLRM` because `GramSVD` is the `pca_method` used when your data is mostly numeric. When your data is mostly categorical you use the `GLRM` `pca_method`. 

## Question

How many initial principal components are chosen with respect to dimensions of the input data? 

**Answer:** There are 42 initial principal components which matches the dimensions of the input data (aka features). 

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()
```

## Question

What category of grocery items contribute most to PC1? (These are related because they're bought most often together on a given grocery trip).

**Answer:** The bulmer cider category contributes the most to PC1.

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```

## Question

What category of grocery items contribute the least to PC1 but positively towards PC2?

**Answer:** The category carrot contributes the least to PC1 but positively toward PC2.

## Eigenvalue criterion

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
```


```{r}
## [1] 42

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```


```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free") 
```

## Question

How many principal components would you include to explain 90% of the total variance?

**Answer:** About 36 principal components explain 90% of the total variance. 

```{r}
# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))
```


```{r}
# Screee plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

## Question

How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?

**Answer:** Include 8 principal components. 

## Question

What are a couple of disadvantages to using PCA? 

**Answer:** PCA are highly influences by outliers and CA does not work well for nonlinear patterns.

# Non-metric MultiDimensional Scaling (NMDS)

## Unconstrained Ordination on Species

```{r}
# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
```

## Question

What are the dimensions of the varespec data frame and what do rows versus columns represent?

**Answer:** The dimensions are 24 rows and 44 columns. The 44 columns represent different species and the 24 rows are sites with estimated cover values in each cell for a species.

```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```

## Question

The “stress” in a stressplot represents the difference between the observed input distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?

**Answer:** The non-metric fit is better than the linear fit by 0.05 in terms of the \(R^2\).


```{r}
ordiplot(vare.mds0, type = "t")
```

## Question

What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?

**Answer:** The two sites that are most dissimilar for MDS1 are sites 28 and 5. For MDS2, the two sites that are most dissimilar are sites 21 and 14.

```{r}
vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds
```

```{r}
plot(vare.mds, type = "t")
```

## Question

What is the basic difference between metaMDS and monoMDS()? 

**Answer:** `metaMDS()` uses `monoMDS()` to create a non-linear regression from several different random starts. 

## Overlay with Environment

```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef
```


```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

## Question

What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?

**Answer:** Aluminium and Iron (Fe) have the strongest negative relationship with NMDS1.

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")
```

## Question

Which of the two NMDS axes differentiates Ca the most, i.e. has the highest value given by the contours at the end (and not middle) of the axis?

**Answer:** NMDS1 differentiates Ca the most.


# Constrained Ordination on Species and Environment

```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```

## Question

What is the difference between “constrained” versus “unconstrained” ordination within ecological context?

**Answer:** In ecology, unconstrained ordination is used to visualize multivariate data. Unconstrained ordination allows us to test hypotheses 

```{r}
# plot ordination
plot(vare.cca)
```


```{r}
# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
```


```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```

## Question

What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environment? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?

**Answer:** Sites 4 and 28 are most differentiated by CCA1. The stronest environmental vector for CCA1 is Al.
